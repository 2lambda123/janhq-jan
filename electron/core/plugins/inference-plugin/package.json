{
  "name": "inference-plugin",
  "version": "1.0.0",
  "description": "Inference Plugin, powered by @janhq/nitro, bring a high-performance Llama model inference in pure C++.",
  "icon": "https://raw.githubusercontent.com/tailwindlabs/heroicons/88e98b0c2b458553fbadccddc2d2f878edc0387b/src/20/solid/command-line.svg",
  "main": "dist/index.js",
  "author": "Jan",
  "license": "MIT",
  "activationPoints": [
    "init"
  ],
  "scripts": {
    "build": "webpack --config webpack.config.js",
    "postinstall": "rimraf ./*.tgz && npm run build && cpx \"module.js\" \"dist\" && rimraf dist/nitro/* && cpx \"nitro/**\" \"dist/nitro\"",
    "build:publish": "npm pack && cpx *.tgz ../../pre-install"
  },
  "devDependencies": {
    "cpx": "^1.5.0",
    "rimraf": "^3.0.2",
    "webpack": "^5.88.2",
    "webpack-cli": "^5.1.4"
  },
  "bundledDependencies": [
    "electron-is-dev",
    "node-llama-cpp"
  ],
  "dependencies": {
    "electron-is-dev": "^2.0.0"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "files": [
    "dist/*",
    "package.json",
    "README.md"
  ]
}
