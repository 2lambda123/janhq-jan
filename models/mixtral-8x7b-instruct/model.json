{
  "source": [
    {
      "filename": "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
      "url": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf"
    }
  ],
  "id": "mixtral-8x7b-instruct",
  "object": "model",
  "name": "Mixtral 8x7B Instruct Q4",
  "version": "1.0",
  "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.",
  "format": "gguf",
  "settings": {
    "ctx_len": 4096,
    "prompt_template": "[INST] {prompt} [/INST]",
    "llama_model_path": "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf"
  },
  "parameters": {
    "max_tokens": 4096
  },
  "metadata": {
    "author": "MistralAI, TheBloke",
    "tags": ["MOE", "Foundational Model"],
    "size": 26440000000
  },
  "engine": "nitro"
}
