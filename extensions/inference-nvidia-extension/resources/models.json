[
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "databricks/dbrx-instruct",
    "object": "model",
    "name": "Databricks DBRX-Instruct",
    "version": "1.1",
    "description": "DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. ",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": [
        "General"
      ]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "mistralai/mistral-7b-instruct-v0.2",
    "object": "model",
    "name": "Mistral 7B",
    "version": "1.1",
    "description": "Mistral 7B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.3,
      "top_p": 1,
      "stream": false
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": [
        "General"
      ]
    },
    "engine": "nvidia"
  }
]
