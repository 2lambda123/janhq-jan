[
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "aisingapore/sea-lion-7b-instruct",
    "object": "model",
    "name": "Sea Lion 7B with NVIDIA",
    "version": "1.1",
    "description": "SEA-LION-7B-Instruct is a multilingual model for natural language understanding (NLU), natural language generation (NLG), and natural language reasoning (NLR) tasks that has been fine-tuned with thousands of English and Indonesian instruction-completion pairs ",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "mistralai/mistral-7b-instruct-v0.2",
    "object": "model",
    "name": "Mistral 7B",
    "version": "1.1",
    "description": "Mistral 7B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.3,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "databricks/dbrx-instruct",
    "object": "model",
    "name": "Databricks",
    "version": "1.1",
    "description": "Databricks with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "google/gemma-7b",
    "object": "model",
    "name": "Gemma 7b",
    "version": "1.1",
    "description": "Gemma 7b with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.3,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "google/gemma-2b",
    "object": "model",
    "name": "Gemma 2b",
    "version": "1.1",
    "description": "Gemma 2b with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "google/codegemma-1.1-7b",
    "object": "model",
    "name": "CodeGemma 1.1-7B",
    "version": "1.1",
    "description": "CodeGemma 1.1-7B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.2,
      "top_p": 0.7,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "google/codegemma-1.1-7b",
    "object": "model",
    "name": "CodeGemma 7B",
    "version": "1.1",
    "description": "CodeGemma 7B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "google/recurrentgemma-2b",
    "object": "model",
    "name": "Recurrent Gemma-2b",
    "version": "1.1",
    "description": "Recurrent Gemma-2b with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.2,
      "top_p": 0.7,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 42
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "ibm/granite-34b-code-instruct",
    "object": "model",
    "name": "Granite 34B",
    "version": "1.1",
    "description": "Granite 34B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "ibm/granite-8b-code-instruct",
    "object": "model",
    "name": "Granite 8B",
    "version": "1.1",
    "description": "Granite 8B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "mediatek/breeze-7b-instruct",
    "object": "model",
    "name": "Breeze 7B",
    "version": "1.1",
    "description": "Breeze 8B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "meta/codellama-70b",
    "object": "model",
    "name": "Codellama 70B",
    "version": "1.1",
    "description": "Codellama 70B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.3,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "meta/llama2-70b",
    "object": "model",
    "name": "llama2 70B",
    "version": "1.1",
    "description": "llama2 70B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 1,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "meta/llama3-8b-instruct",
    "object": "model",
    "name": "llama3 8B",
    "version": "1.1",
    "description": "llama3 8B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "meta/llama3-70b-instruct",
    "object": "model",
    "name": "llama3 70B",
    "version": "1.1",
    "description": "llama3 70B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "microsoft/phi-3-medium-4k-instruct",
    "object": "model",
    "name": "phi-3-medium-4k-instruct",
    "version": "1.1",
    "description": "phi-3-medium-4k-instruct with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.2,
      "top_p": 0.7,
      "stream": false,
      "seed": 42
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "microsoft/phi-3-mini",
    "object": "model",
    "name": "phi-3-mini 128k",
    "version": "1.1",
    "description": "phi-3-mini 128k with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "temperature": 0.2,
      "top_p": 0.7,
      "max_tokens": 1024,
      "seed": 42,
      "stream": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "microsoft/phi-3-mini-4k-instruct",
    "object": "model",
    "name": "phi-3-mini 4k",
    "version": "1.1",
    "description": "phi-3-mini 4k with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "temperature": 0.2,
      "top_p": 0.7,
      "max_tokens": 1024,
      "seed": 42,
      "stream": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "microsoft/phi-3-small-128k-instruct",
    "object": "model",
    "name": "phi-3-small 128k",
    "version": "1.1",
    "description": "phi-3-small 128k with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "temperature": 0.2,
      "top_p": 0.7,
      "max_tokens": 1024,
      "seed": 42,
      "stream": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "microsoft/phi-3-small-8k-instruct",
    "object": "model",
    "name": "phi-3-small 8k",
    "version": "1.1",
    "description": "phi-3-small 8k with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "temperature": 0.2,
      "top_p": 0.7,
      "max_tokens": 1024,
      "seed": 42,
      "stream": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "mistralai/mixtral-8x7b-instruct-v0.1",
    "object": "model",
    "name": "Mistral 8x7B",
    "version": "1.1",
    "description": "Mistral 8x7B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.3,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": null
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "mistralai/mixtral-8x22b-instruct-v0.1",
    "object": "model",
    "name": "Mistral 8x22B",
    "version": "1.1",
    "description": "Mistral 8x22B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "mistralai/mistral-large",
    "object": "model",
    "name": "Mistral large",
    "version": "1.1",
    "description": "Mistral large with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "seallms/seallm-7b-v2.",
    "object": "model",
    "name": "Seallm 7B",
    "version": "1.1",
    "description": "Seallm 7B with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  },
  {
    "sources": [
      {
        "url": "https://integrate.api.nvidia.com/v1/chat/completions"
      }
    ],
    "id": "snowflake/arctic",
    "object": "model",
    "name": "Arctic",
    "version": "1.1",
    "description": "Arctic with NVIDIA",
    "format": "api",
    "settings": {},
    "parameters": {
      "max_tokens": 1024,
      "temperature": 0.5,
      "top_p": 1,
      "stream": false,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "stop": null,
      "seed": 0
    },
    "metadata": {
      "author": "NVIDIA",
      "tags": ["General"]
    },
    "engine": "nvidia"
  }
]
